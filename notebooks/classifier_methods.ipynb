{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbc42f-8818-4aa9-b8a9-7cef1e34fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installs necessary libraries\n",
    "\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install pandas",
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8555d8b-e79c-4fb0-bcf1-c51c0a7edcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
 {
      "cell_type": "code",
      "execution_count": null,
      "id": "690f899e-fe1a-4431-a2b3-d082d6268492",
      "metadata": {
        "id": "690f899e-fe1a-4431-a2b3-d082d6268492"
      },
      "outputs": [],
      "source": [
        "#useful preprocessing methods\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import re\n",
        "\n",
        "#replaces all white space with a single space between words\n",
        "def remove_whitespaces(line):\n",
        "    return \" \".join(line.split())\n",
        "\n",
        "#replaces mispelled words with likely true spellings\n",
        "def spell_correction(text):\n",
        "    wordlist = text.split()\n",
        "    corrected_wordlist = []\n",
        "    checker = SpellCheck()\n",
        "    for word in wordlist:\n",
        "        correct = checker.correction(word)\n",
        "        corrected_wordlist.append(correct)\n",
        "    return \" \".join(corrected_wordlist)\n",
        "\n",
        "#Returns version of text with all words converted to stemmed versions\n",
        "def stemming(text):\n",
        "    wordlist = text.split()\n",
        "    corrected_wordlist = []\n",
        "    porter = PorterStemmer()\n",
        "    for word in wordlist:\n",
        "        correct = porter.stem(word)\n",
        "        corrected_wordlist.append(correct)\n",
        "    return \" \".join(corrected_wordlist)\n",
        "\n",
        "#removes URLs from text\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "#detects if URLs are present in text (can filter out spam posts)\n",
        "def find_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    if re.match(url_pattern, text):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "#removes punctuations from text\n",
        "def remove_punct(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "#returns preprocessed version of text (without stemming)\n",
        "def preprocessv1(text):\n",
        "    text = remove_urls(text)\n",
        "    text = remove_punct(text)\n",
        "    text = remove_whitespaces(text) #removing extra white spaces\n",
        "    #text = stemming(text)\n",
        "    text = text.lower() #lowercasing\n",
        "\n",
        "    return text\n",
        "\n",
        "#returns preprocessed version of text (WITH stemming)\n",
        "def preprocessv2(text):\n",
        "    text = remove_urls(text)\n",
        "    text = remove_punct(text)\n",
        "    text = remove_whitespaces(text) #removing extra white spaces\n",
        "    text = stemming(text)\n",
        "    text = text.lower() #lowercasing\n",
        "    return text"
      ]
    },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8fea8-2833-40ef-a5a8-3a9fb5479f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmed_data(extension):\n",
    "    dfilename = 'dstemmed_' + extension + '.txt'\n",
    "    ufilename = 'ustemmed_' + extension + '.txt'\n",
    "    \n",
    "    ddict = {}\n",
    "    udict = {}\n",
    "    \n",
    "    dfile = open(dfilename,'r')\n",
    "    ufile = open(ufilename,'r')\n",
    "    \n",
    "    for line in dfile:\n",
    "        line_list = line.split(':;')\n",
    "        if len(line_list) != 2:\n",
    "            print('error')\n",
    "            break\n",
    "        ddict[line_list[0]] = line_list[1].strip() \n",
    "    \n",
    "    for line in ufile:\n",
    "        line_list = line.split(':;')\n",
    "        if len(line_list) != 2:\n",
    "            print('error')\n",
    "            break\n",
    "        udict[line_list[0]] = line_list[1].strip() \n",
    "    \n",
    "    dfile.close()\n",
    "    ufile.close()\n",
    "    \n",
    "    return ddict, udict\n",
    "\n",
    "def get_stemmed_data_depression(extension):\n",
    "    ddict, udict = get_stemmed_data(extension)\n",
    "    userlist = get_depression_users()\n",
    "    \n",
    "    ddict, udict = get_stemmed_data(extension)\n",
    "    dusers, uusers = get_depression_users()\n",
    "    \n",
    "    ddict2 = {}\n",
    "    udict2 = {}\n",
    "    for user in dusers:\n",
    "        if user in ddict:\n",
    "            ddict2[user] = ddict[user]\n",
    "    for user in uusers:\n",
    "        if user in udict:\n",
    "            udict2[user] = udict[user]\n",
    "   \n",
    "    return ddict2, udict2\n",
    "\n",
    "def get_stemmed_data_anxiety(extension):\n",
    "    ddict, udict = get_stemmed_data(extension)\n",
    "    dusers, uusers = get_anxiety_users()\n",
    "    \n",
    "    ddict2 = {}\n",
    "    udict2 = {}\n",
    "    for user in dusers:\n",
    "        if user in ddict:\n",
    "            ddict2[user] = ddict[user]\n",
    "    for user in uusers:\n",
    "        if user in udict:\n",
    "            udict2[user] = udict[user]\n",
    "   \n",
    "    return ddict2, udict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78872ea8-caab-4019-870a-300d61af77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_posttimes(anxiety_members, unanxiety_members):\n",
    "    #post time, total number of post, time series stuff\n",
    "    \n",
    "    dusertimes = {}\n",
    "    uusertimes = {}\n",
    "    \n",
    "    dpostnums = {}\n",
    "    upostnums = {}\n",
    "    davgs = {}\n",
    "    uavgs = {}\n",
    "\n",
    "\n",
    "    for user in anxiety_members:\n",
    "        earliest = float('inf')\n",
    "        latest = 0.0\n",
    "        num_posts = 0\n",
    "        time_list = []\n",
    "        \n",
    "        subfilename = 'demoji_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comfilename = 'demoji_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "\n",
    "       \n",
    "        dusertimes[user] = dict.fromkeys(range(24), 0)\n",
    "        subfile = open(subfilename, 'r')\n",
    "        for line in subfile:\n",
    "\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            num_posts += 1\n",
    "         \n",
    "            posttime = float(linelist[2])\n",
    "            time_list.append(posttime)\n",
    "            if posttime < earliest:\n",
    "                earliest = posttime\n",
    "            if posttime > latest:\n",
    "                latest = posttime\n",
    "            dtime = datetime.fromtimestamp(int(float(posttime)))\n",
    "            dhour = dtime.hour\n",
    "            dusertimes[user][dhour] +=1\n",
    "        subfile.close()\n",
    "\n",
    "        comfile = open(comfilename, 'r')\n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 12:\n",
    "                continue\n",
    "            num_posts += 1\n",
    "            posttime = float(linelist[3])\n",
    "            time_list.append(posttime)\n",
    "            if posttime < earliest:\n",
    "                earliest = posttime\n",
    "            if posttime > latest:\n",
    "                latest = posttime\n",
    "           \n",
    "            dtime = datetime.fromtimestamp(int(float(posttime)))\n",
    "            dhour = dtime.hour\n",
    "            dusertimes[user][dhour] += 1\n",
    "\n",
    "        comfile.close()\n",
    "        \n",
    "        dpostnums[user] = num_posts\n",
    "        early_date = datetime.fromtimestamp(int(float(earliest)))\n",
    "        latest_date = datetime.fromtimestamp(int(float(latest)))\n",
    "        delta = latest_date - early_date\n",
    "        denom = float(delta.days)\n",
    "        if int(denom) == 0:\n",
    "            print(f'{user} with {num_posts} has 0 ')\n",
    "            continue\n",
    "        davgs[user] = float(num_posts) / denom\n",
    "\n",
    "\n",
    "    for user in unanxiety_members:\n",
    "       \n",
    "        earliest = float('inf')\n",
    "        latest = 0.0\n",
    "        num_posts = 0\n",
    "        time_list = []\n",
    "        \n",
    "        subfilename = 'demoji_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comfilename = 'demoji_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        \n",
    "        uusertimes[user] = dict.fromkeys(range(24), 0)\n",
    "        subfile = open(subfilename, 'r')\n",
    "        for line in subfile:\n",
    "\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "                \n",
    "            num_posts += 1\n",
    "            posttime = float(linelist[2])\n",
    "            time_list.append(posttime)\n",
    "            if posttime < earliest:\n",
    "                earliest = posttime\n",
    "            if posttime > latest:\n",
    "                latest = posttime\n",
    "            utime = datetime.fromtimestamp(int(float(posttime)))\n",
    "            uhour = utime.hour\n",
    "            uusertimes[user][uhour] +=1\n",
    "        subfile.close()\n",
    "\n",
    "       \n",
    "        comfile = open(comfilename, 'r')\n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 12:\n",
    "                continue\n",
    "            num_posts += 1\n",
    "            posttime = float(linelist[3])\n",
    "            time_list.append(posttime)\n",
    "            if posttime < earliest:\n",
    "                earliest = posttime\n",
    "            if posttime > latest:\n",
    "                latest = posttime\n",
    "\n",
    "            utime = datetime.fromtimestamp(int(float(posttime)))\n",
    "            uhour = utime.hour\n",
    "            uusertimes[user][uhour] +=1\n",
    "        comfile.close()\n",
    "        upostnums[user] = num_posts\n",
    "        early_date = datetime.fromtimestamp(int(float(earliest)))\n",
    "        latest_date = datetime.fromtimestamp(int(float(latest)))\n",
    "        delta = latest_date - early_date\n",
    "        denom = float(delta.days)\n",
    "        if int(denom) == 0:\n",
    "            print(f'{user} with {num_posts} has 0 ')\n",
    "            continue\n",
    "        uavgs[user] = float(num_posts) / denom\n",
    "    return dusertimes, uusertimes, dpostnums, upostnums, davgs, uavgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df34944-3863-47c1-bd1c-834eaa0ad5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anxiety_users():\n",
    "    anxiety_users = []\n",
    "    unanxiety_users = []\n",
    "    \n",
    "    anxietyfile = open('only_anxiety_proven.txt','r')\n",
    "    for user in anxietyfile:\n",
    "        anxiety_users.append(user.strip())\n",
    "   \n",
    "    unanxietyfile = open('only_anxiety_undiagnosed.txt', 'r')\n",
    "    for user in unanxietyfile:\n",
    "        unanxiety_users.append(user.strip())\n",
    "        \n",
    "    return anxiety_users, unanxiety_users\n",
    "\n",
    "def get_anxiety_files():\n",
    "    only_anxiety_dfiles = []\n",
    "    only_anxiety_ufiles = []\n",
    "    \n",
    "    anxietyfile = open('only_anxiety_proven.txt','r')\n",
    "    for user in anxietyfile:\n",
    "    \n",
    "        subfile = 'demoji_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comfile = 'demoji_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        only_anxiety_dfiles.append(subfile)\n",
    "        only_anxiety_dfiles.append(comfile)\n",
    "   \n",
    "    anxietyfile2 = open('only_anxiety_undiagnosed.txt','r')\n",
    "    for user in anxietyfile2:\n",
    "\n",
    "        subfile = 'demoji_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comfile = 'demoji_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        only_anxiety_ufiles.append(subfile)\n",
    "        only_anxiety_ufiles.append(comfile)\n",
    "        \n",
    "    return only_anxiety_dfiles, only_anxiety_ufiles\n",
    "\n",
    "def get_depression_files():\n",
    "    only_depressed_dfiles = []\n",
    "    only_depressed_ufiles = []\n",
    "    \n",
    "    unanxietyfile = open('only_depressed_proven.txt', 'r')\n",
    "    for user in unanxietyfile:\n",
    "\n",
    "        subfile = 'demoji_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comfile = 'demoji_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        only_depressed_dfiles.append(subfile)\n",
    "        only_depressed_dfiles.append(comfile)\n",
    "        \n",
    "    unanxietyfile2 = open('only_depressed_undiagnosed.txt', 'r')\n",
    "    for user in unanxietyfile2:\n",
    "\n",
    "        subfile = 'demoji_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comfile = 'demoji_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        only_depressed_ufiles.append(subfile)\n",
    "        only_depressed_ufiles.append(comfile)\n",
    "    \n",
    "    return only_depressed_dfiles, only_depressed_ufiles\n",
    "\n",
    "def get_depression_users():\n",
    "    depressed_users = []\n",
    "    undepressed_users = []\n",
    "    \n",
    "    unanxietyfile = open('only_depressed_proven.txt', 'r')\n",
    "    for user in unanxietyfile:\n",
    "        depressed_users.append(user.strip())\n",
    "           \n",
    "    unanxietyfile2 = open('only_depressed_undiagnosed.txt', 'r')\n",
    "    for user in unanxietyfile2:\n",
    "        undepressed_users.append(user.strip())\n",
    "        \n",
    "    return depressed_users, undepressed_users\n",
    "       \n",
    "def get_all_users():\n",
    "    all_uusers = []\n",
    "    all_dusers = []\n",
    "    \n",
    "    unanxietyfile = open('proven_all_diagnosed.txt', 'r')\n",
    "    for user in unanxietyfile:\n",
    "        all_dusers.append(user.strip())\n",
    "           \n",
    "    unanxietyfile2 = open('proven_all_undiagnosed.txt', 'r')\n",
    "    for user in unanxietyfile2:\n",
    "        all_uusers.append(user.strip())\n",
    "        \n",
    "    return all_dusers, all_uusers\n",
    "       \n",
    "def get_all_files():\n",
    "    only_depressed_dfiles = []\n",
    "    only_depressed_ufiles = []\n",
    "    \n",
    "    unanxietyfile = open('proven_all_diagnosed.txt', 'r')\n",
    "    for user in unanxietyfile:\n",
    "\n",
    "        subfile = 'demoji_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comfile = 'demoji_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        only_depressed_dfiles.append(subfile)\n",
    "        only_depressed_dfiles.append(comfile)\n",
    "        \n",
    "    unanxietyfile2 = open('proven_all_undiagnosed.txt', 'r')\n",
    "    for user in unanxietyfile2:\n",
    "\n",
    "        subfile = 'demoji_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comfile = 'demoji_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        only_depressed_ufiles.append(subfile)\n",
    "        only_depressed_ufiles.append(comfile)\n",
    "    \n",
    "    return only_depressed_dfiles, only_depressed_ufiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aadf711-1f62-4851-9efe-87ba1d3b260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlist, ulist = get_all_users()\n",
    "dtimesdict, utimesdict, dpostnumdict, upostnumdict, davgdict, uavgdict = get_posttimes(dlist, ulist)\n",
    "\n",
    "sorted_davg = sorted(davgdict.items(), key = lambda x:x[1])\n",
    "sorted_uavg = sorted(uavgdict.items(), key = lambda x:x[1])\n",
    "\n",
    "print(sorted_davg[0])\n",
    "print(sorted_davg[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e034c-533a-4d6a-bd30-c96f25f23148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemming(text):\n",
    "    wordlist = text.split()\n",
    "    corrected_wordlist = []\n",
    "    porter = PorterStemmer()\n",
    "    for word in wordlist:\n",
    "        correct = porter.stem(word)\n",
    "        corrected_wordlist.append(correct)\n",
    "    return \" \".join(corrected_wordlist)\n",
    "\n",
    "def prepare_datasets2(dusers, unusers, stembool):\n",
    "    diagnosed_dict = {}\n",
    "    undiagnosed_dict = {}\n",
    "    \n",
    "    for user in dusers:\n",
    "        subname = 'final_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'final_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        user_string = ''\n",
    "        subfile = open(subname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = stemming(selftext)\n",
    "            user_string = user_string + selftext\n",
    "            user_string = user_string + ' '\n",
    "        subfile.close()\n",
    "        \n",
    "        comfile = open(comname, 'r')\n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist)!= 12:\n",
    "                continue\n",
    "            body = linelist[2]\n",
    "            if stembool:\n",
    "                body = stemming(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        comfile.close()\n",
    "        \n",
    "        diagnosed_dict[user] = user_string\n",
    "        \n",
    "    for user in unusers:\n",
    "        subname = 'final_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'final_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        user_string = ''\n",
    "        subfile = open(subname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = stemming(selftext)\n",
    "            user_string = user_string + selftext\n",
    "            user_string = user_string + ' '\n",
    "        subfile.close()\n",
    "        \n",
    "        comfile = open(comname, 'r')\n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist)!= 12:\n",
    "                continue\n",
    "            body = linelist[2]\n",
    "            if stembool:\n",
    "                body = stemming(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        comfile.close()\n",
    "        \n",
    "        undiagnosed_dict[user] = user_string\n",
    "    \n",
    "    return diagnosed_dict, undiagnosed_dict\n",
    "\n",
    "def prepare_datasets2_excluding(dusers, unusers, stembool, excludinglist):\n",
    "    diagnosed_dict = {}\n",
    "    undiagnosed_dict = {}\n",
    "    \n",
    "    for user in dusers:\n",
    "        subname = 'final_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'final_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        user_string = ''\n",
    "        subfile = open(subname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            subreddit = linelist[10]\n",
    "            if subreddit in excludinglist:\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = stemming(selftext)\n",
    "            user_string = user_string + selftext\n",
    "            user_string = user_string + ' '\n",
    "        subfile.close()\n",
    "        \n",
    "        comfile = open(comname, 'r')\n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist)!= 12:\n",
    "                continue\n",
    "            \n",
    "            subreddit = linelist[9]\n",
    "            if subreddit in excludinglist:\n",
    "                continue\n",
    "                \n",
    "            body = linelist[2]\n",
    "            if stembool:\n",
    "                body = stemming(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        comfile.close()\n",
    "        \n",
    "        diagnosed_dict[user] = user_string\n",
    "        \n",
    "    for user in unusers:\n",
    "        subname = 'final_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'final_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        user_string = ''\n",
    "        subfile = open(subname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            subreddit = linelist[10]\n",
    "            if subreddit in excludinglist:\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = stemming(selftext)\n",
    "            user_string = user_string + selftext\n",
    "            user_string = user_string + ' '\n",
    "        subfile.close()\n",
    "        \n",
    "        comfile = open(comname, 'r')\n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist)!= 12:\n",
    "                continue\n",
    "            \n",
    "            subreddit = linelist[9]\n",
    "            if subreddit in excludinglist:\n",
    "                continue\n",
    "                \n",
    "            body = linelist[2]\n",
    "            if stembool:\n",
    "                body = stemming(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        comfile.close()\n",
    "        \n",
    "        undiagnosed_dict[user] = user_string\n",
    "    \n",
    "    return diagnosed_dict, undiagnosed_dict\n",
    "\n",
    "def prepare_datasets2_including(dusers, unusers, stembool, includinglist):\n",
    "    diagnosed_dict = {}\n",
    "    undiagnosed_dict = {}\n",
    "    \n",
    "    for user in dusers:\n",
    "        subname = 'final_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'final_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        user_string = ''\n",
    "        subfile = open(subname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            subreddit = linelist[10]\n",
    "            if not subreddit in includinglist:\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = stemming(selftext)\n",
    "            user_string = user_string + selftext\n",
    "            user_string = user_string + ' '\n",
    "        subfile.close()\n",
    "        \n",
    "        comfile = open(comname, 'r')\n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist)!= 12:\n",
    "                continue\n",
    "            \n",
    "            subreddit = linelist[9]\n",
    "            if not subreddit in includinglist:\n",
    "                continue\n",
    "                \n",
    "            body = linelist[2]\n",
    "            if stembool:\n",
    "                body = stemming(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        comfile.close()\n",
    "        \n",
    "        diagnosed_dict[user] = user_string\n",
    "        \n",
    "    for user in unusers:\n",
    "        subname = 'final_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'final_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        user_string = ''\n",
    "        subfile = open(subname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            subreddit = linelist[10]\n",
    "            if not subreddit in includinglist:\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = stemming(selftext)\n",
    "            user_string = user_string + selftext\n",
    "            user_string = user_string + ' '\n",
    "        subfile.close()\n",
    "        \n",
    "        comfile = open(comname, 'r')\n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist)!= 12:\n",
    "                continue\n",
    "            \n",
    "            subreddit = linelist[9]\n",
    "            if not subreddit in includinglist:\n",
    "                continue\n",
    "                \n",
    "            body = linelist[2]\n",
    "            if stembool:\n",
    "                body = stemming(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        comfile.close()\n",
    "        \n",
    "        undiagnosed_dict[user] = user_string\n",
    "    \n",
    "    return diagnosed_dict, undiagnosed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f16b0-ca5b-4258-9d09-5e027906d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(dusers, unusers, stembool):\n",
    "    diagnosed_dict = {}\n",
    "    undiagnosed_dict = {}\n",
    "    temp_list = [] #delete\n",
    "    for user in dusers:\n",
    "        user_string = ''\n",
    "        subname = 'demoji_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'demoji_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        subfile = open(subname, 'r')\n",
    "        comfile = open(comname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                if not user in temp_list:\n",
    "                    temp_list.append(user)\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = preprocessv2(selftext)\n",
    "            else:\n",
    "                selftext = preprocessv1(selftext)\n",
    "            user_string = user_string + selftext \n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 12:\n",
    "                if not user in temp_list:\n",
    "                    temp_list.append(user)\n",
    "                continue\n",
    "            body  = linelist[2]\n",
    "            if stembool:\n",
    "                body = preprocessv2(body)\n",
    "            else:\n",
    "                body = preprocessv1(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        diagnosed_dict[user] = user_string\n",
    "        subfile.close()\n",
    "        comfile.close()\n",
    "    \n",
    "    for user in unusers:\n",
    "        user_string = ''\n",
    "        subname = 'demoji_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'demoji_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        subfile = open(subname, 'r')\n",
    "        comfile = open(comname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = preprocessv2(selftext)\n",
    "            else:\n",
    "                selftext = preprocessv1(selftext)\n",
    "           \n",
    "            user_string = user_string + selftext \n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 12:\n",
    "                continue\n",
    "            body  = linelist[2]\n",
    "            if stembool:\n",
    "                body = preprocessv2(body)\n",
    "            else:\n",
    "                body = preprocessv1(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        undiagnosed_dict[user] = user_string\n",
    "        subfile.close()\n",
    "        comfile.close()\n",
    "    print(len(temp_list))\n",
    "    print(temp_list)\n",
    "    return diagnosed_dict, undiagnosed_dict\n",
    "\n",
    "def prepare_datasets_exclude(dusers, unusers, stembool, excluding):\n",
    "    diagnosed_dict = {}\n",
    "    undiagnosed_dict = {}\n",
    "    \n",
    "    for user in dusers:\n",
    "        user_string = ''\n",
    "        subname = 'demoji_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'demoji_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        subfile = open(subname, 'r')\n",
    "        comfile = open(comname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            subreddit = linelist[10]\n",
    "            if subreddit in excluding:\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = preprocessv2(selftext)\n",
    "            else:\n",
    "                selftext = preprocessv1(selftext)\n",
    "            user_string = user_string + selftext \n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 12:\n",
    "                continue\n",
    "            subreddit = linelist[9]\n",
    "            if subreddit in excluding:\n",
    "                continue\n",
    "            body  = linelist[2]\n",
    "            if stembool:\n",
    "                body = preprocessv2(body)\n",
    "            else:\n",
    "                body = preprocessv1(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        diagnosed_dict[user] = user_string\n",
    "        subfile.close()\n",
    "        comfile.close()\n",
    "    \n",
    "    for user in unusers:\n",
    "        user_string = ''\n",
    "        subname = 'demoji_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'demoji_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        subfile = open(subname, 'r')\n",
    "        comfile = open(comname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = preprocessv2(selftext)\n",
    "            else:\n",
    "                selftext = preprocessv1(selftext)\n",
    "           \n",
    "            user_string = user_string + selftext \n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 12:\n",
    "                continue\n",
    "            body  = linelist[2]\n",
    "            if stembool:\n",
    "                body = preprocessv2(body)\n",
    "            else:\n",
    "                body = preprocessv1(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        undiagnosed_dict[user] = user_string\n",
    "        subfile.close()\n",
    "        comfile.close()\n",
    "    return diagnosed_dict, undiagnosed_dict\n",
    "    \n",
    "def prepare_datasets_include(dusers, uunsers, stembool, including):\n",
    "    diagnosed_dict = {}\n",
    "    undiagnosed_dict = {}\n",
    "    \n",
    "    for user in dusers:\n",
    "        user_string = ''\n",
    "        subname = 'demoji_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'demoji_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        subfile = open(subname, 'r')\n",
    "        comfile = open(comname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            subreddit = linelist[10]\n",
    "            if not subreddit in including:\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = preprocessv2(selftext)\n",
    "            else:\n",
    "                selftext = preprocessv1(selftext)\n",
    "            user_string = user_string + selftext \n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 12:\n",
    "                continue\n",
    "            subreddit = linelist[9]\n",
    "            if not subreddit in including:\n",
    "                continue\n",
    "            body  = linelist[2]\n",
    "            if stembool:\n",
    "                body = preprocessv2(body)\n",
    "            else:\n",
    "                body = preprocessv1(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        diagnosed_dict[user] = user_string\n",
    "        subfile.close()\n",
    "        comfile.close()\n",
    "    \n",
    "    for user in unusers:\n",
    "        user_string = ''\n",
    "        subname = 'demoji_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "        comname = 'demoji_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "        subfile = open(subname, 'r')\n",
    "        comfile = open(comname, 'r')\n",
    "        for line in subfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 15:\n",
    "                continue\n",
    "            selftext = linelist[9]\n",
    "            selftext += ' '\n",
    "            selftext += linelist[14]\n",
    "            if stembool:\n",
    "                selftext = preprocessv2(selftext)\n",
    "            else:\n",
    "                selftext = preprocessv1(selftext)\n",
    "           \n",
    "            user_string = user_string + selftext \n",
    "            user_string = user_string + ' '\n",
    "        \n",
    "        for line in comfile:\n",
    "            linelist = line.split(':;')\n",
    "            if len(linelist) != 12:\n",
    "                continue\n",
    "            body  = linelist[2]\n",
    "            if stembool:\n",
    "                body = preprocessv2(body)\n",
    "            else:\n",
    "                body = preprocessv1(body)\n",
    "            user_string = user_string + body\n",
    "            user_string = user_string + ' '\n",
    "        undiagnosed_dict[user] = user_string\n",
    "        subfile.close()\n",
    "        comfile.close()\n",
    "    return diagnosed_dict, undiagnosed_dict\n",
    "\n",
    "def prepare_dataframe(ddict, udict):\n",
    "    undiagnosed_frame1 = pd.DataFrame(udict, index = [0])\n",
    "    utframe1 = undiagnosed_frame1.T\n",
    "    uclass = [0] * len(utframe1)\n",
    "    utframe1[1] = uclass\n",
    "\n",
    "    diagnosed_frame1 = pd.DataFrame(ddict, index = [0])\n",
    "    dtframe1 = diagnosed_frame1.T\n",
    "    dclass = [1] * len(dtframe1)\n",
    "    dtframe1[1] = dclass\n",
    "    \n",
    "    totalframe = dtframe1.append(utframe1)\n",
    "    print(f'dframe size = {dtframe1.size}')\n",
    "    print(f'utframe size = {utframe1.size}')\n",
    "    print(f'totalframe size = {totalframe.size}')\n",
    "    return totalframe\n",
    "\n",
    "def get_stemmed_data(extension):\n",
    "    dfilename = 'dstemmed_' + extension + '.txt'\n",
    "    ufilename = 'ustemmed_' + extension + '.txt'\n",
    "    \n",
    "    ddict = {}\n",
    "    udict = {}\n",
    "    \n",
    "    dfile = open(dfilename,'r')\n",
    "    ufile = open(ufilename,'r')\n",
    "    \n",
    "    for line in dfile:\n",
    "        line_list = line.split(':;')\n",
    "        if len(line_list) != 2:\n",
    "            print('error')\n",
    "            break\n",
    "        ddict[line_list[0]] = line_list[1].strip() \n",
    "    \n",
    "    for line in ufile:\n",
    "        line_list = line.split(':;')\n",
    "        if len(line_list) != 2:\n",
    "            print('error')\n",
    "            break\n",
    "        udict[line_list[0]] = line_list[1].strip() \n",
    "    \n",
    "    dfile.close()\n",
    "    ufile.close()\n",
    "    \n",
    "    return ddict, udict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5c8b2-bb5f-4e8b-b011-c0dd2a008318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#block with model methods\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def MB_hypertune(text_clf, xtrain,ytrain):\n",
    "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False),'clf__alpha': (1, 1e-1, 1e-2, 1e-3),}\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, scoring = 'f1')\n",
    "    gs_clf = gs_clf.fit(xtrain, ytrain)\n",
    "    print(gs_clf.best_score_)\n",
    "    print(gs_clf.best_params_)\n",
    "    return gs_clf.best_score_ ,gs_clf.best_params_\n",
    "\n",
    "def MB_hypertune2(text_clf, xtrain,ytrain):\n",
    "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],'clf__alpha': (1, 1e-1, 1e-2, 1e-3),}\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, scoring = 'f1')\n",
    "    gs_clf = gs_clf.fit(xtrain, ytrain)\n",
    "    print(gs_clf.best_score_)\n",
    "    print(gs_clf.best_params_)\n",
    "    return gs_clf.best_score_ ,gs_clf.best_params_\n",
    "\n",
    "def SGD_hypertune(text_clf, xtrain, ytrain):\n",
    "    parameters = {'vect__ngram_range': [(1,1), (1,2)], 'tfidf__use_idf': (True, False),'clf__alpha': (1, 1e-1, 1e-2, 1e-3), 'clf__loss': ('hinge', 'modified_huber', 'squared_hinge', 'perceptron')}\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, scoring = 'f1')\n",
    "    gs_clf = gs_clf.fit(xtrain, ytrain)\n",
    "    print(gs_clf.best_score_)\n",
    "    print(gs_clf.best_params_)\n",
    "    return gs_clf.best_score_ ,gs_clf.best_params_\n",
    "\n",
    "def SGD_hypertune2(text_clf, xtrain, ytrain):\n",
    "    parameters = {'vect__ngram_range': [(1,1), (1,2)],'clf__alpha': (1, 1e-1, 1e-2, 1e-3), 'clf__loss': ('hinge', 'modified_huber', 'squared_hinge', 'perceptron')}\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, scoring = 'f1')\n",
    "    gs_clf = gs_clf.fit(xtrain, ytrain)\n",
    "    print(gs_clf.best_score_)\n",
    "    print(gs_clf.best_params_)\n",
    "    return gs_clf.best_score_ ,gs_clf.best_params_\n",
    "\n",
    "def MNB_custom(dframe, vectorizer):\n",
    "    text_MLB = Pipeline(([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()) ]))\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(dframe[0], dframe[1], test_size = 0.34, random_state = 41, shuffle = True)\n",
    "    ytrainsplit = {}\n",
    "    ytestsplit = {}\n",
    "    for num in ytrain:\n",
    "        if num not in ytrainsplit:\n",
    "            ytrainsplit[num] = 1\n",
    "        else:\n",
    "            ytrainsplit[num] += 1\n",
    "    for num in ytest:\n",
    "        if num not in ytestsplit:\n",
    "            ytestsplit[num] = 1\n",
    "        else:\n",
    "            ytestsplit[num] += 1\n",
    "    print('y train split')\n",
    "    print(ytrainsplit)\n",
    "    print('y test split')\n",
    "    print(ytestsplit)\n",
    "    xtrain2 = vectorizer.fit_transform(xtrain)\n",
    "    xtest2 = vectorizer.transform(xtest)\n",
    "    MNBmodel = MultinomialNB()\n",
    "    \n",
    "    MNBmodel.fit(xtrain2, ytrain)\n",
    "    text_MLB.fit(xtrain, ytrain)\n",
    "    best_score, best_params = MB_hypertune(text_MLB,xtrain, ytrain)\n",
    "    #evaluation\n",
    "    labels = ['Diagnosed', 'Undiagnosed']\n",
    "    scores = cross_val_score(text_MLB, xtest, ytest, cv=10)\n",
    "    acc = scores.mean()\n",
    "    print(\"10-fold Cross Validation Accuracy: %0.2f percent\" % (acc *100))\n",
    "    \n",
    "   # predictions = test_MLB.predict(xtest)\n",
    "    predictions = MNBmodel.predict(xtest2)\n",
    "    predicto = {}\n",
    "    for num in predictions:\n",
    "        if num not in predicto:\n",
    "            predicto[num] = 1\n",
    "        else:\n",
    "            predicto[num] += 1\n",
    "    print('predicto')\n",
    "    print(predicto)\n",
    "    \n",
    "    print(\"\\nConfusion matrix\")\n",
    "    print(confusion_matrix(ytest, predictions))\n",
    "    \n",
    "    print(\"\\nClassification report\")\n",
    "    print(classification_report(ytest, predictions))\n",
    "   \n",
    "    \n",
    "def MNB_count(dframe):\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=200000, stop_words='english')\n",
    "    return MNB_custom(dframe, vectorizer)\n",
    "def MNB_tfidf(dframe):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=200000, stop_words='english')\n",
    "    return MNB_custom(dframe, vectorizer)\n",
    "\n",
    "def SGD_custom(dframe, vectorizer):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(dframe[0], dframe[1], shuffle = True)\n",
    "    xtrain2 = vectorizer.fit_transform(xtrain)\n",
    "    xtest2 = vectorizer.transform(xtest)\n",
    "   # SGDmodel = make_pipeline(StandardScaler(with_mean=False), SGDClassifier(max_iter = 1000,tol=1e-3))\n",
    "    SGDmodel = SGDClassifier(max_iter = 1000,tol=1e-3)\n",
    "    SGDmodel.fit(xtrain2, ytrain)\n",
    "    #evaluation\n",
    "    labels = ['Diagnosed', 'Undiagnosed']\n",
    "    scores = cross_val_score(SGDmodel, xtest2, ytest, cv=10)\n",
    "    acc = scores.mean()\n",
    "    print(\"10-fold Cross Validation Accuracy: %0.2f percent\" % (acc *100))\n",
    "    \n",
    "    predictions = SGDmodel.predict(xtest2)\n",
    "    print(\"\\nConfusion matrix\")\n",
    "    print(confusion_matrix(ytest, predictions))\n",
    "    \n",
    "    print(\"\\nClassification report\")\n",
    "    print(classification_report(ytest, predictions))\n",
    "\n",
    "def SGD_count(dframe):\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=200000, stop_words='english')\n",
    "    return SGD_custom(dframe, vectorizer)\n",
    "def SGD_tfidf(dframe):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=200000, stop_words='english')\n",
    "    return SGD_custom(dframe, vectorizer)\n",
    "\n",
    "def MNB(dframe, tfbool, ngrams, alpha_value):\n",
    "    if tfbool:\n",
    "        text_MLB = Pipeline(([('vect', CountVectorizer(ngram_range = ngrams)), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB(alpha = alpha_value)) ]))\n",
    "    else:\n",
    "        text_MLB = Pipeline(([('vect', CountVectorizer(ngram_range = ngrams)), ('clf', MultinomialNB(alpha = alpha_value)) ]))\n",
    "    \n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(dframe[0], dframe[1], test_size = 0.34, random_state = 41, shuffle = True)\n",
    "    ytrainsplit = {}\n",
    "    ytestsplit = {}\n",
    "    for num in ytrain:\n",
    "        if num not in ytrainsplit:\n",
    "            ytrainsplit[num] = 1\n",
    "        else:\n",
    "            ytrainsplit[num] += 1\n",
    "    for num in ytest:\n",
    "        if num not in ytestsplit:\n",
    "            ytestsplit[num] = 1\n",
    "        else:\n",
    "            ytestsplit[num] += 1\n",
    "    print('y train split')\n",
    "    print(ytrainsplit)\n",
    "    print('y test split')\n",
    "    print(ytestsplit)\n",
    "    text_MLB.fit(xtrain, ytrain)\n",
    "    labels = ['Diagnosed', 'Undiagnosed']\n",
    "    scores = cross_val_score(text_MLB, xtest, ytest, cv=10)\n",
    "    acc = scores.mean()\n",
    "    print(\"10-fold Cross Validation Accuracy: %0.2f percent\" % (acc *100))\n",
    "    predictions = text_MLB.predict(xtest)\n",
    "    \n",
    "    print(\"\\nConfusion matrix\")\n",
    "    print(confusion_matrix(ytest, predictions))\n",
    "    \n",
    "    print(\"\\nClassification report\")\n",
    "    print(classification_report(ytest, predictions))\n",
    "    return text_MLB\n",
    "def SGD(dframe, tfbool, ngrams, alpha_value, loss_func):\n",
    "    if tfbool:\n",
    "        text_SGD = Pipeline(([('vect', CountVectorizer(ngram_range = ngrams)), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier(alpha = alpha_value, loss = loss_func, max_iter = 1000)) ]))\n",
    "    else:\n",
    "        text_SGD = Pipeline(([('vect', CountVectorizer(ngram_range = ngrams)), ('clf', SGDClassifier(alpha = alpha_value, loss =loss_func, max_iter = 1000)) ]))\n",
    "    \n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(dframe[0], dframe[1], test_size = 0.34, random_state = 41, shuffle = True)\n",
    "    ytrainsplit = {}\n",
    "    ytestsplit = {}\n",
    "    for num in ytrain:\n",
    "        if num not in ytrainsplit:\n",
    "            ytrainsplit[num] = 1\n",
    "        else:\n",
    "            ytrainsplit[num] += 1\n",
    "    for num in ytest:\n",
    "        if num not in ytestsplit:\n",
    "            ytestsplit[num] = 1\n",
    "        else:\n",
    "            ytestsplit[num] += 1\n",
    "    print('y train split')\n",
    "    print(ytrainsplit)\n",
    "    print('y test split')\n",
    "    print(ytestsplit)\n",
    "    text_SGD.fit(xtrain, ytrain)\n",
    "    labels = ['Diagnosed', 'Undiagnosed']\n",
    "    scores = cross_val_score(text_SGD, xtest, ytest, cv=10)\n",
    "    acc = scores.mean()\n",
    "    print(\"10-fold Cross Validation Accuracy: %0.2f percent\" % (acc *100))\n",
    "    predictions = text_SGD.predict(xtest)\n",
    "    \n",
    "    print(\"\\nConfusion matrix\")\n",
    "    print(confusion_matrix(ytest, predictions))\n",
    "    \n",
    "    print(\"\\nClassification report\")\n",
    "    print(classification_report(ytest, predictions))\n",
    "    return text_SGD\n",
    "    \n",
    "def SGD_tune(dframe):\n",
    "    text_SGD = Pipeline(([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier(max_iter = 1000)) ]))\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(dframe[0], dframe[1], test_size = 0.34, random_state = 41, shuffle = True)\n",
    "    text_SGD.fit(xtrain, ytrain)\n",
    "    best_score, best_params = SGD_hypertune(text_SGD,xtrain, ytrain)\n",
    "    \n",
    "def SGD_tune2(dframe):\n",
    "    text_SGD = Pipeline(([('vect', CountVectorizer()), ('clf', SGDClassifier(max_iter = 1000)) ]))\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(dframe[0], dframe[1], test_size = 0.34, random_state = 41, shuffle = True)\n",
    "    text_SGD.fit(xtrain, ytrain)\n",
    "    best_score, best_params = SGD_hypertune2(text_SGD,xtrain, ytrain)\n",
    "    \n",
    "def MNB_tune(dframe):\n",
    "    text_MLB = Pipeline(([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()) ]))\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(dframe[0], dframe[1], test_size = 0.34, random_state = 41, shuffle = True)\n",
    "    text_MLB.fit(xtrain, ytrain)\n",
    "    best_score, best_params = MB_hypertune(text_MLB,xtrain, ytrain)\n",
    "    \n",
    "def MNB_tune2(dframe):\n",
    "    text_MLB = Pipeline(([('vect', CountVectorizer()),  ('clf', MultinomialNB()) ]))\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(dframe[0], dframe[1], test_size = 0.34, random_state = 41, shuffle = True)\n",
    "    text_MLB.fit(xtrain, ytrain)\n",
    "    best_score, best_params = MB_hypertune2(text_MLB,xtrain, ytrain)\n",
    "    \n",
    "#choice 1 is generic, 2 is excluding list1, 3 is including list1 \n",
    "def tune_all(choice, list1):\n",
    "    dlist_all, ulist_all = get_stemmed_data('all')\n",
    "    dlist_anxiety, ulist_anxiety = get_stemmed_data('anxiety')\n",
    "    dlist_depression, ulist_depression = get_stemmed_data('depressed')\n",
    "    \n",
    "    if choice == 1:\n",
    "      #  ddict_all, udict_all = get_stemmed_data('all')\n",
    "       # dframe_all = prepare_dataframe(ddict_all, udict_all)\n",
    "       # MNB_tune(dframe_all)\n",
    "        \n",
    "       # ddict_anxiety, udict_anxiety = get_stemmed_data('anxiety')\n",
    "       # dframe_anxiety = prepare_dataframe(ddict_anxiety, udict_anxiety)\n",
    "       # MNB_tune(dframe_anxiety)\n",
    "        \n",
    "        dict_depression, udict_depression = get_stemmed_data('depressed')\n",
    "        dframe_depression = prepare_dataframe(dict_depression, udict_depression)\n",
    "        MNB_tune(dframe_depression)\n",
    "        \n",
    "    elif choice == 2:\n",
    "        ddict_all, udict_all = get_stemmed_data('all')\n",
    "        dframe_all = prepare_dataframe(ddict_all, udict_all)\n",
    "        MNB_tune(dframe_all)\n",
    "        \n",
    "        ddict_anxiety, udict_anxiety = get_stemmed_data('anxiety')\n",
    "        dframe_anxiety = prepare_dataframe(ddict_anxiety, udict_anxiety)\n",
    "        MNB_tune(dframe_anxiety)\n",
    "        \n",
    "        dict_depression, udict_depression = get_stemmed_data('depressed')\n",
    "        dframe_depression = prepare_dataframe(dict_depression, udict_depression)\n",
    "        MNB_tune(dframe_depression)\n",
    "    elif choice == 3:\n",
    "        ddict_all, udict_all = get_stemmed_data('all')\n",
    "        dframe_all = prepare_dataframe(ddict_all, udict_all)\n",
    "        MNB_tune(dframe_all)\n",
    "        \n",
    "        ddict_anxiety, udict_anxiety = get_stemmed_data('anxiety')\n",
    "        dframe_anxiety = prepare_dataframe(ddict_anxiety, udict_anxiety)\n",
    "        MNB_tune(dframe_anxiety)\n",
    "        \n",
    "        dict_depression, udict_depression = get_stemmed_data('depressed')\n",
    "        dframe_depression = prepare_dataframe(dict_depression, udict_depression)\n",
    "        MNB_tune(dframe_depression)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e0119-bde0-4a39-9cfe-b5a4734880c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allddict, alludict = get_stemmed_data_anxiety('topical')\n",
    "ddframe = prepare_dataframe(allddict, alludict)\n",
    "SGD_tune(ddframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55fc3a-e050-4a78-906c-255ca7c4fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "allddict, alludict = get_stemmed_data_depression('topical')\n",
    "ddframe = prepare_dataframe(allddict, alludict)\n",
    "SGD(ddframe, True, (1,2), 0.01, 'perceptron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e8db4-937b-472c-9c7b-b96c17fb7faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "allddict, alludict = get_stemmed_data_anxiety('topical')\n",
    "ddframe = prepare_dataframe(allddict, alludict)\n",
    "MNB_tune2(ddframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8a9fc-d928-420a-8c39-b69ef29af5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allddict, alludict = get_stemmed_data_anxiety('topical')\n",
    "ddframe = prepare_dataframe(allddict, alludict)\n",
    "MNB(ddframe, False, (1,1), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e5852-d54f-4136-8a74-a77e4c03f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stemmed_data():\n",
    "    dlist, ulist = get_all_users()\n",
    "    danxiety, uanxiety = get_anxiety_users()\n",
    "    ddepressed, udepressed = get_depression_users()\n",
    "    \n",
    "    ddict, udict = prepare_datasets(dlist, ulist, True)\n",
    "    \n",
    "    dall_file = open('dstemmed_all.txt', 'w')\n",
    "    danxiety_file = open('dstemmed_anxiety.txt', 'w')\n",
    "    ddepressed_file = open('dstemmed_depressed.txt', 'w')\n",
    "    \n",
    "    for user in ddict:\n",
    "        dall_file.write(user)\n",
    "        dall_file.write(':;')\n",
    "        dall_file.write(ddict[user])\n",
    "        dall_file.write('\\n')\n",
    "        \n",
    "        if user in danxiety:\n",
    "            danxiety_file.write(user)\n",
    "            danxiety_file.write(':;')\n",
    "            danxiety_file.write(ddict[user])\n",
    "            danxiety_file.write('\\n')\n",
    "            \n",
    "        if user in ddepressed:\n",
    "            ddepressed_file.write(user)\n",
    "            ddepressed_file.write(':;')\n",
    "            ddepressed_file.write(ddict[user])\n",
    "            ddepressed_file.write('\\n')\n",
    "    \n",
    "    dall_file.close()\n",
    "    danxiety_file.close()\n",
    "    ddepressed_file.close()\n",
    "    \n",
    "    uall_file = open('ustemmed_all.txt', 'w')\n",
    "    uanxiety_file = open('ustemmed_anxiety.txt', 'w')\n",
    "    udepressed_file = open('ustemmed_depressed.txt', 'w')\n",
    "    \n",
    "    for user in udict:\n",
    "        uall_file.write(user)\n",
    "        uall_file.write(':;')\n",
    "        uall_file.write(udict[user])\n",
    "        uall_file.write('\\n')\n",
    "        \n",
    "        if user in uanxiety:\n",
    "            uanxiety_file.write(user)\n",
    "            uanxiety_file.write(':;')\n",
    "            uanxiety_file.write(udict[user])\n",
    "            uanxiety_file.write('\\n')\n",
    "            \n",
    "        if user in udepressed:\n",
    "            udepressed_file.write(user)\n",
    "            udepressed_file.write(':;')\n",
    "            udepressed_file.write(udict[user])\n",
    "            udepressed_file.write('\\n')\n",
    "    uall_file.close()\n",
    "    uanxiety_file.close()\n",
    "    udepressed_file.close()\n",
    "    \n",
    "def write_stemmed_data2(extension, includinglist):\n",
    "    dlist, ulist = get_all_users()\n",
    "    ddict, udict = prepare_datasets2_including(dlist, ulist, True, includinglist)\n",
    "    \n",
    "    dwritefilename = 'dstemmed_'+extension + '.txt'\n",
    "    dwritefile = open(dwritefilename, 'w')\n",
    "    for user in ddict:\n",
    "        dwritefile.write(user)\n",
    "        dwritefile.write(':;')\n",
    "        dwritefile.write(ddict[user])\n",
    "        dwritefile.write('\\n')\n",
    "    dwritefile.close()\n",
    "    \n",
    "    uwritefilename = 'ustemmed_'+extension + '.txt'\n",
    "    \n",
    "    uwritefile = open(uwritefilename,'w')\n",
    "    for user in udict:\n",
    "        uwritefile.write(user)\n",
    "        uwritefile.write(':;')\n",
    "        uwritefile.write(udict[user])\n",
    "        uwritefile.write('\\n')\n",
    "    uwritefile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7c685-e484-4d45-9d24-cfbfea9885ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "\n",
    "def sentiment_dataset(ddict, udict):\n",
    "    \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "  #  scores = sia.polarity_scores()\n",
    "    dsentiments = {}\n",
    "    usentiments = {}\n",
    "    \n",
    "    for user in ddict:\n",
    "        scores = sia.polarity_scores(ddict[user])\n",
    "        break\n",
    "    \n",
    "    for user in udict:\n",
    "        break\n",
    "    \n",
    "    \n",
    "sentiment_dataset(anxiety_files, unanxiety_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68778b5c-715d-42e8-bdea-8791cdcc610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill data structures\n",
    "\n",
    "filelist = ['general_discussion.txt','hobbies.txt','topical_discussion.txt','mental_health_and_support.txt','physical_health_and_wellness.txt']\n",
    "keylist = ['General', 'Hobbies', 'Topical Discussion', 'Mental Health and Support', 'Physical Health and Wellness']\n",
    "finaldict = {}\n",
    "exclusive_list = []\n",
    "\n",
    "for index in range(len(filelist)):\n",
    "    finaldict[keylist[index]] = []\n",
    "    tempfile = open(filelist[index],'r')\n",
    "   \n",
    "    for line in tempfile:\n",
    "        finaldict[keylist[index]].append(line.strip())\n",
    "        if not keylist[index] == 'Mental Health and Support':\n",
    "            exclusive_list.append(line.strip())\n",
    "    tempfile.close()\n",
    "general_list = finaldict['General']\n",
    "hobbies_list = finaldict['Hobbies']\n",
    "topical_list = finaldict['Topical Discussion']\n",
    "mental_list = finaldict['Mental Health and Support']\n",
    "physical_list = finaldict['Physical Health and Wellness']\n",
    "\n",
    "print(len(general_list))\n",
    "print(len(hobbies_list))\n",
    "print(len(topical_list))\n",
    "print(len(mental_list))\n",
    "print(len(physical_list))\n",
    "print(len(exclusive_list))\n",
    "#print(exclusive_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad1ca9-747c-4ad7-b285-de3764b9b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_stemmed_data2('exclusive', exclusive_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575f8a8-00e0-4bc1-8e4d-873f7f5a0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_anxiety_dfiles = []\n",
    "only_depressed_dfiles = []\n",
    "only_anxiety_ufiles = []\n",
    "only_depressed_ufiles = []\n",
    "\n",
    "anxietyfile = open('only_anxiety_proven.txt','r')\n",
    "for user in anxietyfile:\n",
    "    \n",
    "    subfile = 'demoji_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "    comfile = 'demoji_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "    only_anxiety_dfiles.append(subfile)\n",
    "    only_anxiety_dfiles.append(comfile)\n",
    "    \n",
    "unanxietyfile = open('only_depressed_proven.txt', 'r')\n",
    "for user in unanxietyfile:\n",
    "    \n",
    "    subfile = 'demoji_diagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "    comfile = 'demoji_diagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "    only_depressed_dfiles.append(subfile)\n",
    "    only_depressed_dfiles.append(comfile)\n",
    "    \n",
    "anxietyfile2 = open('only_anxiety_undiagnosed.txt','r')\n",
    "for user in anxietyfile2:\n",
    "    \n",
    "    subfile = 'demoji_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "    comfile = 'demoji_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "    only_anxiety_ufiles.append(subfile)\n",
    "    only_anxiety_ufiles.append(comfile)\n",
    "    \n",
    "unanxietyfile2 = open('only_depressed_undiagnosed.txt', 'r')\n",
    "for user in unanxietyfile2:\n",
    "    \n",
    "    subfile = 'demoji_undiagnosed/' + user.strip() + '_submissions_stripped.txt'\n",
    "    comfile = 'demoji_undiagnosed/' + user.strip() + '_comments_stripped.txt'\n",
    "    only_depressed_ufiles.append(subfile)\n",
    "    only_depressed_ufiles.append(comfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b731b35-078b-46c6-9949-1b3d41fb93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set device\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 25, kernel_size=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(25, 50)\n",
    "      \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the parameters\n",
    "size = 3\n",
    "filters = 25\n",
    "pool_length = \"all\"\n",
    "dense_layers = 1\n",
    "dense_dim = 50\n",
    "dropout = 0.0\n",
    "class_balance = \"sampled\"\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = CNN()#.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Print the model summary\n",
    "print(model)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = loss_function(batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Example usage of the model\n",
    "input_data = torch.randn(1, 3, size, size).to(device)\n",
    "output = model(input_data)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e7d4c-5d21-4ab0-8d5f-32e2b8e0d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext classifier stuff\n",
    "def fast_boi(extension):\n",
    "    trainpath = 'fasttext_' + extension + '_train.txt'\n",
    "    testpath =  'fasttext_' + extension + '_test.txt'\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier = fasttext.train_supervised(\n",
    "        input=trainpath,\n",
    "        lr=0.1,\n",
    "        epoch=25,\n",
    "        wordNgrams=2,\n",
    "        bucket=200000,\n",
    "        dim=50,\n",
    "        loss='softmax'\n",
    "    )\n",
    "    \n",
    "    result = classifier.test(testpath)\n",
    "    print(f'Results: {result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
